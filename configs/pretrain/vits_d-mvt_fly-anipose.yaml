# model configuration
model:
  name: mvt
  seed: 0
  checkpoint: null  # load weights from checkpoint
  model_class: vit
  pretrained: facebook/dino-vits16
  model_params:
    hidden_size: 384
    num_hidden_layers: 12
    num_attention_heads: 6
    intermediate_size: 1536
    hidden_act: "gelu"
    hidden_dropout_prob: 0.0
    attention_probs_dropout_prob: 0.0
    initializer_range: 0.02
    layer_norm_eps: 1.e-12
    image_size: 224 # usually 224
    patch_size: 16 # default is 16, we use large patch size
    num_channels: 3 # 3 for RGB
    qkv_bias: True
    decoder_num_attention_heads: 16
    decoder_hidden_size: 512
    decoder_num_hidden_layers: 8
    decoder_intermediate_size: 2048
    mask_ratio: 0.75 # 0 for no masking, usually 0.75 (MAE)
    norm_pix_loss: False
    decoder_num_channels: 4 # 3 for RGB, 1 for depth


# training configuration
training:
  seed: 0
  imgaug: default
  train_batch_size: 32    # per GPU
  val_batch_size: 1024
  test_batch_size: 128
  num_epochs: 800
  num_workers: 16  # Number of CPU workers for the DataLoader
  num_gpus: 1
  num_nodes: 1
  # frequency to log training metrics
  log_every_n_steps: 10
  log_every_n_epochs: 20
  # frequency to log validation metrics
  check_val_every_n_epoch: 5
  resume: null
  resume_from_best: false
  # frequency to save model
  save_every_n_epochs: 100
  save_plots: false
  save_plots_every_n_epochs: 10

  effective_batch_size: 1024 # if null, use the default value from the optimizer

  use_wandb: false
  use_bfloat16: true

# optimizer configuration
optimizer:
  type: AdamW
  accumulate_grad_batches: 1
  lr: 5.e-5
  wd: 0.05
  warmup_pct: 0.15 # cosine/linear
  gamma: 0.95     # step
  div_factor: 10  # cosine
  scheduler: cosine # step/cosine/linear

# data configuration
data:
  name: mvt
  data_dir: data/ssl/fly-anipose
  output_mod:
    [
      "rgb",
      "depth",
    ]